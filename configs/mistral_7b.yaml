# Configuration for Mistral 7B
# Best for: Highest quality outputs
# Memory: ~7-8GB VRAM with QLoRA (TIGHT FIT!)
# Recommended for: RTX 2080 (8GB) - ADVANCED, requires careful tuning

# Model configuration
model_name: "mistralai/Mistral-7B-v0.1"
use_4bit: true
use_8bit: false
bnb_4bit_compute_dtype: "bfloat16"  # Use "float16" if bf16 not supported
bnb_4bit_quant_type: "nf4"
use_nested_quant: true

# LoRA parameters (keeping it minimal to save memory)
lora_r: 8  # Reduced from 16 to save memory
lora_alpha: 16  # Reduced proportionally
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  # Only training attention layers to save memory
  # Uncomment below to train MLP layers (needs more memory)
  # - "gate_proj"
  # - "up_proj"
  # - "down_proj"

# Training parameters (aggressive memory optimization)
output_dir: "./results/mistral-7b-finetuned"
num_train_epochs: 3
per_device_train_batch_size: 1  # MUST be 1 for 8GB GPU
gradient_accumulation_steps: 8  # Effective batch size = 8
learning_rate: 2.0e-4
max_grad_norm: 0.3
weight_decay: 0.001
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Memory optimization (CRITICAL for 8GB)
gradient_checkpointing: true
max_seq_length: 512  # Reduced to save memory (can try 1024 if it fits)

# Logging and saving
logging_steps: 10
save_steps: 100
eval_steps: 100
save_total_limit: 2  # Reduced to save disk space

# Dataset configuration (update these)
dataset_name: null  # e.g., "tatsu-lab/alpaca" or set train_file below
dataset_text_field: "text"
train_file: null  # e.g., "./data/train.json"
validation_file: null  # e.g., "./data/val.json"

# Misc
seed: 42
use_wandb: false  # Set to true to enable Weights & Biases logging
wandb_project: "llm-finetuning"
wandb_run_name: "mistral-7b-run"

# ⚠️  IMPORTANT NOTES FOR RTX 2080 (8GB):
# 1. This config is OPTIMIZED for tight memory constraints
# 2. If you get OOM errors, try:
#    - Reduce max_seq_length to 256 or 384
#    - Ensure no other programs are using GPU memory
#    - Reduce lora_r to 4
# 3. Monitor GPU memory: watch -n 1 nvidia-smi
# 4. Consider using Mistral-7B-Instruct for instruction tuning
